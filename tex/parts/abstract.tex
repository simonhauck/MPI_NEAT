% !TeX spellcheck = de_DE
\section*{Zusammenfassung}
Neuroevolutionäre Algorithmen werden im Vergleich zu klassischen Verfahren wie dem Backpropagation Algorithmus weniger häufig zum Optimieren von künstlichen neuronalen Netzen eingesetzt. Allerdings sind Algorithmen dieser Art gut parallelisierbar, was im Bereich \emph{High Performance Computing} ein entscheidender Vorteil sein kann. In dieser Arbeit wird der \acs*{NEAT} Algorithmus zuerst sequenziell implementiert und die benötigte Ausführungszeit in verschiedenen Optimierungsproblemen analysiert. Auf Basis der dabei erhaltenen Ergebnisse wird eine parallelisierte Implementierung mit \acs*{MPI} erstellt. Das Verfahren wird auf einem Beowulf Cluster evaluiert und die Ausführungszeit im Vergleich zum sequenziellen Verfahren bewertet. Die Ergebnisse zeigen eine stark reduzierte Ausführungszeit und insgesamt sehr gute Effizienzwerte.

% !TeX spellcheck = de_DE
\section*{Abstract}
Neuroevolutionary algorithms are used less frequently to optimize artificial neural networks compared to more traditional methods like the backpropagation algorithm. However, algorithms of this type can easily be parallelized, which can be an important advantage in the field of\emph{High Performance Computing}. In this thesis a sequential version of the NEAT Algorithm is implemented and the required execution time is analyzed in various optimization problems. Based on the obtained results, a parallelized implementation with MPI is created. The new implementation will be evaluated on a Beowulf cluster and the execution time is compared to the sequential implementation. The results show a greatly reduced execution time and overall very good efficiency values.
%\begin{abstract}
%	Neuroevolutionäre Algorithmen sind ein mögliches Optimierungsverfahren für neuronale Netze. Abhängig von dem verwendeten Algorithmus können die Gewichte der Verbindungen im Netz und die Struktur entwickelt und optimiert werden.\\
%	Der Optimierungsprozess ist, unabhängig vom Verfahren, sehr aufwändig und dementsprechend zeit- und rechenintensiv. Für eine schnellere Durchführung des Trainingsprozesses bieten sich Algorithmen an, die gut parallelisierbar sind. Die benötigte Ausführungszeit dieser kann durch Hinzufügen weiterer Rechenknoten mit geringem Aufwand maßgeblich reduziert werden.\\
%	Neuroevolutionäre Algorithmen bieten sich aufgrund der Verfahrensweise und der vielen unabhängigen neuronalen Netzen für eine parallele Ausführung an.\\
%	In dieser Arbeit wird, stellvertretend für neuroevolutionäre Algorithmen, der \ac*{NEAT} Algorithmus betrachtet. Dieser wurde im Jahr 2002 veröffentlicht und ist im Vergleich zu den damals bekannten Algorithmen besonders effizient. Zudem dient der Algorithmus als Grundlage für viele Erweiterungen. Die erhaltenen Ergebnisse dieser Arbeit lassen sich somit gut auf ebendiese Erweiterungen übertragen.\\
%	Im ersten Schritt dieser Arbeit wird die Laufzeit des \acs*{NEAT} Algorithmus mit verschiedenen Optimierungsaufgaben analysiert. Mit den erhaltenen Ergebnissen wird eine parallelisierte Implementierung erstellt. Diese führt mit unterschiedlich vielen Rechenknoten dieselben Optimierungsaufgaben durch. Am Ende dieser Arbeit werden die Ergebnisse von beiden Implementierungen verglichen.	
%\end{abstract}