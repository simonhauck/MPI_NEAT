% !TeX spellcheck = de_DE
\section{Implementierung} % TODO CHECK in diesem Schritt
In diesem Abschnitt wird die praktische Umsetzung der Parallelisierung betrachtet. Das Ziel ist, den bereits in Kapitel \ref{subsec:mpi} vorgestellten Standard \ac{MPI} für die Kommunikation zu verwenden und die bereits definierten Schnittstellen und Serviceklassen wiederzuverwenden. Dies ermöglicht das einfache Austauschen der beiden Implementierungen und die vorgestellten Optimierungsprobleme können in derselben Implementierung erneut ausgeführt werden. Grundsätzlich dient als Basis für die parallelisierte Version, die in Kapitel \ref{sec:sequential_implementation} vorgestellte, sequenzielle Implementierung. Da der Fokus auf der Phase Evaluation liegt, können die restlichen Bestandteile in diesem Schritt übernommen werden.
\\\\
Im bisherigen Verfahren wurde bei der Evaluation über die Liste mit allen Agenten iteriert und für jeden nacheinander die Evaluation durchgeführt. Wie bereits beschrieben, wird hierzu vor jedem neuen Agenten die Umgebung einmalig zurück gesetzt, das \ac{KNN} gebildet und letztendlich die eigentliche Evaluation durchgeführt mit dem Ziel am Ende einen Fitnesswert zurückzugeben. Bei der parallelisierten Implementierung sollen die Agenten wie im vorherigen Kapitel beschrieben an alle Prozesse verteilt werden, welche dann die Evaluation durchführen. 
\\\\
Daher muss im nächsten Schritt die Art der Kommunikation und Verteilung der Agenten festgelegt werden. In Kapitel \ref{subsec:mpi} sind die \emph{Scatter} und \emph{Gather} Funktion vorgestellt, welche sich prinzipiell für dieses Szenario anbieten. Die \emph{Scatter} Funktion verteilt die Agenten gleichmäßig auf die Prozesse und die \emph{Gather} Funktion sammelt die Ergebnisse. Zwar ist dies effizient zu implementieren aber es ergibt sich ein Nachteil. Die \emph{Scatter} Funktion erlaubt keine dynamische Verteilungen von Lasten, welche in diesem Projekt aus zwei Gründen sinnvoll ist. Der erste betrifft die Evaluationszeiten und der zweite die Struktur des Clusters. Die Evaluationszeiten von Agenten unterscheiden sich in vielen Fällen. Gründe hierfür können einerseits unterschiedlich große \ac{KNN} sein deren Berechnungszeit bzw. Aktivierungszeit variiert aber auch unterschiedliche Fortschritte im Optimierungsproblem. In der vorgestellten \emph{Mountain Car} Umgebung wird beispielsweise die Evaluation beendet, wenn der Agent das Ziel erreicht. In anderen Optimierungsproblemen kann die Evaluation frühzeitig abgebrochen werden, weil ein Agent eine gewisse Fehlentscheidung getroffen hat. Die Folge in beiden Szenarien ist, dass im Vergleich zu anderen Prozessen weniger Rechenaufwand für die Evaluation des Agenten benötigt wird. Hierdurch entsteht ein Ungleichgewicht in der Lastenverteilung. Die Folge ist, dass am Ende einer Evaluationsphase die Prozesse aufeinander warten und die Effizient der Parallelisierung und in Folge dessen auch der \emph{SpeedUp} sinken werden. Selbst wenn alle Agenten dieselbe Rechenlast erzeugen und diese gleichmäßig auf alle Prozesse verteilt ist, kann es in einem Beowulf Cluster zu Wartezeiten kommen. Grund hierfür ist, dass unterschiedliche \acp{CPU} für die einzelnen Geräte verwendet werden, deren Leistung sich stark unterscheiden kann. So ist für eine gute Performance eines parallelisierten Verfahren nötig, dass Prozesse, welche die doppelte Rechenleistung besitzen, auch die doppelte Rechenlast zugewiesen bekommen. Dies ist mit der \emph{Scatter} Funktion nicht möglich.
\\\\
Stattdessen soll in dieser Arbeit die \emph{Point-to-Point} Kommunikation verwendet, mit welcher ein dynamisches Zuteilen grundsätzlich möglich ist. Um mögliche \emph{Deadlocks} zu vermeiden wird eine \emph{Master-Slave} Architektur verwendet. Zu Beginn der Evaluationsphase erstellt der \emph{Master} Prozess verschiedene Arbeitspakete, welche jeweils einen zu evaluierenden Agenten enthalten. Initial wird an jeden \emph{Slave} Prozess ein Paket asynchron gesendet. Die \emph{Slave} Prozesse warten initial auf ein Arbeitspaket. Wird dieses empfangen, wird der darin enthaltene Agent im lokalen Optimierungsproblem evaluiert und der berechnete Fitnesswert als Ergebnis zurück an den \emph{Master} Prozess gesendet. Beim Empfangen eines Ergebnisses wird der \emph{Master} Prozess dieses speichern und überprüfen ob noch weitere Arbeitspakete zur Abarbeitung ausstehen. Ist dies der Fall, wird dieses an den \emph{Slave} übermittelt, welcher erneut mit der Abarbeitung beginnt. Ein großer Vorteil dieses Verfahrens ist, dass Prozesse automatisch mehr Arbeitspakete zugeteilt wenn sie entweder leistungsfähiger sind oder die Agenten kürzere Evaluationszeiten haben.
\\\\
Da dieser Implementierungsansatz sehr bekannt und verbreitet ist, wird er auch standardmäßig vom Paket \emph{mpi4py} unterstützt und die Funktionalität wird in dieser Arbeit verwendet. 

% MPI4Py
% Wie kommen Worker an Umgebung