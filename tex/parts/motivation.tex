% !TeX spellcheck = de_DE
\chapter{Motivation}
Mit \acp{KNN} sind in  den letzten Jahren wissenschaftliche Durchbrüche in verschiedenen Themengebieten erzielt worden. Entsprechend groß ist das Interesse an diesem Forschungsgebiet, auch in der allgemeinen Öffentlichkeit. So hat beispielsweise der Nutzer \emph{SethBling} im Jahr 2015 auf der Plattform \emph{YouTube} ein Video veröffentlicht, das zum Zeitpunkt dieser Arbeit fast zehn Millionen mal aufgerufen und zu $98\%$ positiv bewertet ist. In diesem Video wurde der \ac{NEAT} Algorithmus vorgestellt, der zum Optimieren von \ac{KNN} genutzt wird \cite{bling2015MarIO}.
\\\\
Für den großen Erfolg von \ac{KNN} gibt es mehrere Gründe. Einer davon ist, dass es sich prinzipiell um eine mathematische Abbildungsvorschrift handelt, die Eingabewerte mithilfe einfacher Rechenoperationen zu Ausgabewerten konvertiert. Dieses Konzept kann durch unterschiedliche Konfigurationen leicht für diverse Aufgabengebiete angepasst werden. Der dabei entstehende Implementierungsaufwand ist meistens gering. Für den erfolgreichen Einsatz von \ac{KNN} wird zusätzlich ein Optimierungsverfahren benötigt. Dieses wird in der sogenannten Lern- oder Trainigsphase auf das \ac{KNN} angewendet. Dabei sollen die internen Parameter so verändert werden, dass die Ausgabewerte den gewünschten Ergebnissen entsprechen. 
\\\\
Prinzipiell gibt es verschiedene Optimierungsverfahren, mit den unterschiedliche Ansätze und Schwerpunkte umgesetzt werden können. Ein sehr bekanntes und häufig genutztes Verfahren ist der Backpropagation Algorithmus, welcher zu den gradientenbasierten Verfahren gehört. Eine Alternative hierzu sind die von der natürlichen Evolution inspirierten neuroevolutionären Algorithmen. Bei diesen Verfahren wird ein künstliche Evolution mit dem Ziel simuliert, einn \ac{KNN} zu optimieren. Dabei werden diverse Begriffe wie Selektion, Rekombination und Mutation vom biologischen Vorbild übernommen. Einer der bekanntesten Vertreter neuroevolutionärer Algorithmen ist der bereits genannte \ac{NEAT} Algorithmus. Dieser hat im Vergleich zu anderen Algorithmen beeindruckende Ergebnisse erzielt und dient als Basis für viele Erweiterungen.

\section{Ziel der Arbeit}
Das Optimieren eines \ac{KNN}ist bezüglich des zeitlichen Aspekts sehr aufwendig. Trainingszeiten von mehreren Stunden oder Tagen können notwendig sein, bis das \ac{KNN} eine ausreichende Genauigkeit erzielt. Neuroevolutionäre Algorithmen sind diesbezüglich keine Ausnahme. Das Simulieren einer künstlichen Evolution benötigt häufig längere Laufzeiten als alternative Verfahren, wie zum Beispiel der Backpropagation Algorithmus \cite{whitley1993genetic}. Dies ist mitunter ein Grund, warum neuroevolutionäre Algorithmen bedeutend seltener eingesetzt werden als gradientenbasierte Verfahren. Ein Vorteil der neuroevolutionären Algorithmen ist, dass sie gut parallelisierbar sind \cite{such2017deep}. Im Bereich von \ac{HPC} mit vielen zur Verfügung stehenden unabhängigen Prozessoren kann dies ein entscheidender Vorteil sein, da durch das Hinzufügen weiterer Rechenressourcen die Ausführungszeit verringert werden kann. Dies ist häufig eine der kosteneffizientesten Möglichkeiten, die Ausführungszeit von Algorithmen zu reduzieren. Trifft diese Eigenschaft auf neuroevolutionäre Algorithmen zu, können sie einen Vorteil gegenüber anderen Verfahren bieten.
\\\\
Das Ziel dieser Arbeit ist dreigeteilt. Zuerst soll das Optimierungsverfahren \ac{NEAT} stellvertretend für neuroevolutionäre Algorithmen in der Sprache Python als sequenzielles Verfahren implementiert werden. Hierfür ist eine geeignete Bibliothek zu erstellen, die eine Schnittstelle für verschiedene Optimierungsprobleme bietet. Im zweiten Schritt ist eine Analyse des Verfahrens anhand mehrerer Beispiele durchzuführen. Die hierfür benötigten Ausführungszeiten müssen erfasst und ausgewertet werden. Auf Basis der Ergebnisse ist im dritten Schritt eine geeignete Strategie für die Parallelisierung zu wählen und zu implementieren. Danach sollen die zuvor verwendeten Beispiele wiederholt werden. Zuletzt ist eine Beurteilung über die Effizienz abzugeben. Eine Anforderung an das parallelisierte Verfahren ist, dass es für den Bereich des \ac{HPC} geeignet sein soll. Daher wird für die Kommunikation innerhalb der parallelisierten Implementierung der weit verbreitete Standard \ac{MPI} verwendet. 

%Eine wichtige Komponente im Bezug zu \ac{KNN} sind die sogenannten Optimierungsverfahren. Um die richtige Lösung für Eingabewerte zu produzieren, müssen die internen Parameter eines \ac{KNN} angepasst werden. Dies ist in größeren Systemen nicht manuell möglich und wird mit einem Optimierungsverfahren realisiert.


% Universelle Function 

%\\\\
%\ac{KNN} gehören zum Forschungsgebiet des maschinellen Lernens und werden zum Lösen von verschiedenen Optimierungsproblemen eingesetzt. Typischerweise wird hierbei zwischen der Nutzungs- und Trainingsphase unterschieden. In der Trainingsphase werden die internen Parameter des \acp{KNN} mit einem Optimierungsverfahren angepasst, mit dem Ziel gute Lösungen für das spezifizierte Problem zu finden. Erzielt das \ac{KNN} die gewünschten Ergebnisse mit einer entsprechenden Genauigkeit, beginnt die Nutzungsphase. In dieser wird das optimierte \ac{KNN} in einer produktiven Umgebung eingesetzt. Die internen Parameter werden in dieser Phase im Regelfall nicht mehr angepasst. 
%\\\\
%Insgesamt bietet der Einsatz von \ac{KNN} einige entscheidende Vorteile. Die grundlegenden Komponenten der \ac{KNN} und die dazugehörigen Optimierungsverfahren können mit einer entsprechenden Konfiguration für viele verschiedene Optimierungsprobleme angepasst werden. Generell ist dies durch den geringen Implementierungsaufwand effizient umzusetzen. Die Ergebnisse der \ac{KNN} sind in vielen Fällen außerordentlich gut. Unter anderem in den Bereichen Klassifizierung, Spracherkennung und dem selbständigen Lösen von Computer- und Gesellschaftsspielen sind mit den \ac{KNN} wissenschaftliche Durchbrüche erzielt worden und aber auch das öffentliche Interesse an diesem Forschungsgebiet ist stark gestiegen.
%\\\\
%Der hierbei entstehende Implementierungsaufwand ist gering, sodass eine effiziente Entwicklung  eine effiziente Entwicklung wird ermöglicht. Zudem sind die erhaltenen Ergebnisse in vielen Fällen außerordentlich gut.
%\\\\
%
%
%Dadurch ist der Implementierungsaufwand gering und eine effiziente Entwicklung  Dadurch ist eine effiziente Entwicklung möglich. Des weiteren sind die erzielten Ergebnisse in vielen Fällen sehr gut
%\\\\
%Der große Erfolg von \ac{KNN} entsteht unter anderem durch bieten einige entscheidende Vorteile, welche die Nutzung in verschiedenen Bereichen ermöglicht. Die grundlegende Struktur und Funktionsweise ist 
%
%
% Unter anderem in den Bereichen Klassifizierung, Spracherkennung und dem selbständigen Lösen von Computer- und Gesellschaftsspielen sind mit den \ac{KNN} wissenschaftliche Durchbrüche erzielt worden und aber auch das öffentliche Interesse an diesem Forschungsgebiet ist stark gestiegen. 2015 hat der Nutzer \emph{SethBling} auf der Plattform \emph{YouTube} ein Video veröffentlicht, in welchem er den Einsatz von dem Algorithmus \acs*{NEAT} zum Lösen des bekannten Spiels

\section{Struktur der Arbeit}
In Kapitel \ref{chap:basics} werden die benötigten Grundlagen vorgestellt. Zuerst wird auf den Aufbau und die Funktionsweise von \ac{KNN} eingegangen. Im Anschluss sind die \ac{EA} beschrieben. Dafür werden zunächst die allgemeinen Konzepte vorgestellt, danach wird aufgezeigt, wie diese auf neuroevolutionäre Algorithmen übertragen werden. Der dritte Teil der Grundlagen stellt den \ac{NEAT} Algorithmus im Detail vor, welcher stellvertretend für die neuroevolutionären Algorithmen verwendet wird. Der letzte Teil der Grundlagen befasst sich mit \ac{HPC} und der Vorstellung des Protokolls \ac{MPI}, welches für die Parallelisierung verwendet werden soll. In Kapitel \ref{chap:software_architecture} wird die Softwarearchitektur vorgestellt. Dies umfasst eine Beschreibung der verwendeten Klassen und Schnittstellen. Zudem wird im Rahmen dieses Kapitels eine sequenzielle Implementierung von \ac{NEAT} erstellt. In Kapitel \ref{chap:analysis} wird diese analysiert. Dabei wird zuerst die korrekte Funktionalität verifiziert und danach die Laufzeit des Verfahrens in verschiedenen Beispielen erfasst und ausgewertet. Auf Basis der dabei erhaltenen Ergebnisse wird in Kapitel \ref{chap:optimization} das Verfahren mit \ac{MPI} parallelisiert und die Laufzeit erneut gemessen. Zuletzt wird die Effizienz der Parallelisierung im Vergleich zum sequenziellen Verfahren bewertet. Der erstellte Programmcode, die verwendeten Beispielprojekte und Diagramme sind auf \emph{GitHub} unter folgendem Link verfügbar:
\begin{center}
	\url{https://github.com/simonhauck/MPI_NEAT}
\end{center}



\acresetall