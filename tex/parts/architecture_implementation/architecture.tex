% !TeX spellcheck = de_DE
\section{Softwarearchitektur}
Im Folgenden wird auf die Architektur der Software sowie der Implementierung des sequenziellen Verfahrens eingegangen. Das grundsätzliche Ziel ist, dass so viele Komponenten wie möglich von der sequentiellen und parallelisierten Implementierung genutzt werden können, sodass einerseits der programmiertechnische Aufwand möglichst gering ist und andererseits ein einfaches Austauschen der beiden Varianten möglich ist. Hierfür wird eine Kombination aus verschiedenen Interfaces sowie Serviceklassen verwendet, auf welche im Rahmen dieses Kapitels genauer eingegangen wird. Zuvor werden noch die verwendeten Datenstrukturen und das Interface für die Bibliothek vorgestellt.
 
\subsubsection{Datenstrukturen}
\label{subsubsec:data_structures}
Die verwendeten Datenstrukturen sind für beide Implementierungen identisch und in Abbildung (TODO ABBILDUNG) in einem UML Diagramm dargestellt. Wie zu erkennen ist, besitzen diese keine Funktionen und können so für beide Implementierungen genutzt werden. Die eigentliche Funktionalität wird in den später vorgestellten Serviceklassen umgesetzt.
\\\\
Die erste Struktur ist das \emph{Genom}, welches aus Kapitel \ref{sec:neat} bekannt ist und alle Informationen zur Konstruktion eines \ac{KNN} besitzt. Hierfür werden zwei Listen benötigt, wobei die erste alle Neuronen und die zweite alle Verbindungen des \ac{KNN} enthält. Diese Repräsentation entspricht der Kodierung aus Kapitel \ref{subsec:neat_encoding}. 
\\\\
Ein Neuron wird durch die Klasse \emph{Node} repräsentiert und enthält unter anderem den Bias-Wert und die zu verwendende Aktivierungsfunktion, welche für die Berechnungen im \ac{KNN} benötigt werden. Zusätzlich sind einige weitere Werte vorhanden, welche die spätere Implementierung der Anforderungen vereinfachen. Hierzu gehört unter anderem der Neuronentyp, welcher angibt, ob es sich um ein \emph{Input}-, \emph{Hidden}- oder \emph{Ouput}-Neuron handelt. Dies vereinfacht die spätere Visualisierung sowie das Einsetzen der Eingabewerte und das Auslesen des Ausgabevektors. Zusätzlich besitzt jedes Neuron eine X-Koordinate, welche die relative X-Position für eine spätere Visualisierung angibt. Die \emph{Input}- und \emph{Output}-Neuronen besitzen die Werte $0$ und $1$. Die \emph{Hidden}-Neuronen werden, wie in Kapitel \ref{subsec:neat_minimal_structure} beschrieben, nur durch strukturelle Mutationen hinzugefügt und dabei zwischen zwei anderen Neuronen platziert. Die X-Koordinate von diesen wird berechnet, indem der Mittelwert der zwei anderen Neuronen gebildet wird. Zuletzt enthält jedes Neuron zur Identifizierung eine ID, welche von den Verbindungen genutzt wird.
\\\\
Eine Verbindung zwischen zwei Neuronen wird im Genom durch eine Instanz der Klasse \emph{Connection} repräsentiert. Jede von diesen enthält zwei Werte, welche die ID des Start- und Zielneurons der Verbindung enthalten. Zusätzlich ist das Gewicht sowie ein Aktivierungsbit gegeben. Letzteres gibt entsprechend der vorgestellten Kodierung an, ob die Verbindung im \ac{KNN} enthalten sein soll. Die letzte Variable ist die Innovationsnummer, welche in Kapitel \ref{subsec:neat_reproduction} beschrieben ist. Diese wird durch strukturelle Mutationen zugewiesen und in der Reproduktionsphase zum Lösen des \emph{Competing Conventions} Problems verwendet.
\\\\
In Kapitel \ref{sec:evolutionary_algos} ist der Begriff des Individuums erläutert. In dieser Arbeit werden diese durch Instanzen der Klasse \emph{Agent} repräsentiert und sind nach dem Agenten benannt, welcher beim bestärkenden Lernen mit der Umwelt interagiert. Der hierbei erhaltene Fitnesswert, welcher im späteren Verlauf die Selektion maßgeblich beeinflusst, wird in der entsprechenden Variable \emph{fitness} gespeichert. Das Feld \emph{additional\_info} kann Zusatzinformationen enthalten, welche vom Optimierungsproblem stammen können.
\\\\
Die zwei letzten Klassen in diesem Diagramm sind die \emph{Generation} und \emph{Spezies}. Letzteres ist aus dem \ac{NEAT} Kapitel bekannt und ist eine Gruppierung von ähnlichen Individuen bzw. Agenten. Entsprechend der in Kapitel \ref{subsec:neat_species} enthaltenen Definition ist eine Spezies durch ein Genom repräsentiert und besitzt eine Mitgliederliste. Zusätzlich wird der höchste erreichte Fitnesswert des besten Mitglieds gespeichert sowie die Generation, in welcher dieser Wert erzielt wurde. Dies wird benötigt, um den Fortschritt zu überwachen, da eine Spezies nicht für die Reproduktion ausgewählt wird, deren Mitglieder keine Steigerung des Fitnesswertes in einer festgelegten Anzahl an Generationen erzielen.
\\\\
Die letzte Klasse ist die \emph{Generation}, welche aus Kapitel \ref{subsec:evolutionary_algorithm} bekannt ist. Diese enthält sowohl eine Liste mit allen Individuen als auch eine der verschiedenen Spezies. Zusätzlich wird diese durch eine Nummer identifiziert, welche angibt, wie viele Zyklen der Evaluation, Selektion, Rekombination und Mutation bereits durchgeführt wurden.

\subsubsection{Künstliches neuronales Netz}
Aus dem Genom wird ein \ac{KNN} erstellt, welches dann im Optimierungsproblem eingesetzt wird. In Kapitel \ref{sec:neuroal_networks} sind der Aufbau und die Funktionsweise von diesen ausführlich erläutert. Prinzipiell können hierfür verschiedene bereits implementierte Bibliotheken verwendet werden, wie beispielsweise Tensorflow \cite{tensorflow2015} oder Pytorch \cite{pytorch2019}. Mit diesen können vor allem rein ebenenweise verbundene \ac{KNN}, wie in Kapitel \ref{subsec:network_structures} vorgestellt, schnell und einfach erstellt werden. Allerdings können durch \ac{NEAT} auch \ac{KNN} mit \emph{shortcut} Verbindungen entstehen sowie Netze mit Rückkopplungen. Zwar können diese  auch mit Tensorflow und Pytorch abgebildet werden, aber dies erfordert einen bedeutend höheren Aufwand. Da dies im Rahmen dieser Arbeit nicht möglich ist, wird eine eigene vereinfachte Implementierung verwendet. Ein Nachteil von dieser ist, dass sie im Vergleich zu Implementierungen aus großen Bibliotheken insgesamt langsamer ist. Der Grund hierfür ist, dass die Bibliotheken gut optimiert sind und auch die Verwendung von \acp{GPU} unterstützen, welche die benötigte Ausführungszeit stark reduzieren können. Auch wenn diese nicht verwendet werden, wird im Folgenden ein Grundgerüst erstellt, sodass eine spätere Integration einfach möglich ist. Dies wird mit einem Interface realisiert, welches die grundlegenden Funktionen definiert. Sowohl die in dieser Arbeit verwendete Implementierung als auch zukünftige Erweiterungen mit Tensorflow oder ähnlichem können dieses nutzen und ermöglichen somit ein einfaches Austauschen der verschiedenen Implementierungen. Das Interface ist in Abbildung (TODO ABBILDUNG) dargestellt und definiert drei Funktionen mit den Namen \emph{build()}, \emph{reset()} und \emph{activate()}. Bei der \emph{build()} Funktion wird ein Genom übergeben und hieraus ein \ac{KNN} konstruiert. Die \emph{activate()} Funktion bekommt eine Liste mit Eingabewerte übergeben und produziert den Ausgabevektor des \ac{KNN}. Die Anzahl an Eingabe- und Ausgabewerten entspricht der Anzahl an \emph{Input}- und \emph{Output}-Neuronen im \ac{KNN}. Bei der \emph{reset()} Funktion werden eventuell gespeicherte Ergebnisse, wie sie bei einem Netz mit Rückkopplungen enthalten sein können, entfernt und auf den Startwert zurückgesetzt. 
\\\\
Die in dieser Arbeit verwendete \emph{BasicNeuralNetwork} Implementierung setzt dieses Interface mit dem Ziel um, dass sowohl Netze mit und ohne Rückkopplungen umgesetzt werden können. Da eine genaue Beschreibung der Implementierung nicht von Interesse ist, wird an dieser Stelle nur der oberflächliche Ablauf beschrieben. Bei der \emph{build()} Funktion wird je eine Liste für die \emph{Input}- und \emph{Output}-Neuronen angelegt, in welche die Neuronen mit den entsprechenden Typen sortiert werden. Eine dritte Liste enthält alle im \ac{KNN} enthaltenen Neuronen. Jedes Neuron speichert zudem eine Liste mit den Verbindungen, welche zu ihm führen und den aktuellen sowie letzten berechneten Wert. Die letzte Aufgabe dieser Funktion ist die Festlegung der Reihenfolge, in welcher die einzelnen Neuronen aktiviert bzw. deren Zwischenergebnisse berechnet werden. Bei der Reihenfolge ist zu beachten, dass bei einem Netz ohne Rückkopplung jedes vorherige Neuron bis zu den \emph{Input-Neuronen} bereits aktiviert sein muss, da ansonsten das Ergebnis verfälscht wird. Bei Netzen mit Rückkopplungen gilt diese Anforderung prinzipiell auch, außer für Verbindungen, welche in derselben Schicht oder von Neuronen der nachfolgenden Schichten ausgehen. Dies sind Rückkopplungen, bei welchen der zuletzt berechnete Wert zurückgeben wird, sodass keine Endlosschleife entsteht. 
\\\\
Bei der \emph{activate()} Funktion wird ein Eingabevektor an das \ac{KNN} übergeben. Jeder darin enthaltene Wert wird in ein \emph{Input}-Neuron gesetzt. Dies ist mit der zuvor in der \emph{build()} Funktion erstellten Liste mit allen \emph{Input}-Neuronen effizient umzusetzen. Danach beginnt die eigentliche Berechnung des Ausgabevektors. Hierzu werden die Neuronen in der zuvor festgelegten Reihenfolge aktiviert bzw. berechnet. Zuletzt wird der Ausgabevektor erstellt. Auch dies ist effizient umsetzbar, da zuvor ein Liste mit allen \emph{Output}-Neuronen erstellt wurde. Über diese wird iteriert und die entsprechenden Ergebnisse in eine neue Liste kopiert, welche schlussendlich den Ausgabevektor repräsentiert und als Ergebnis der Funktion zurückgegeben wird. 
\\\\
Die \emph{reset()} Funktion iteriert über die Liste mit allen Neuronen und entfernt die gespeicherten Zwischenergebnisse. 

\subsubsection{Optimierungsproblem}
In den vorherigen Kapiteln sind die grundlegenden Datenstrukturen und die Funktionalität des \ac{KNN} vorgestellt. Bevor der eigentliche Optimierungsalgorithmus implementiert werden kann, wird noch eine grundlegende Komponente benötigt und zwar das Optimierungsproblem. Wie in Kapitel \ref{subsub:optimzation_problem} vorgestellt, können diese aus verschiedenen Domänen stammen und sich sehr unterscheiden. Somit muss auch für dieses ein Interface mit dem Ziel erstellt werden, möglichst viele Szenarien abbilden zu können, welche einfach und mit wenig Aufwand durch das in dieser Arbeit implementierte Verfahren optimierbar sind.
\\\\
Das Interface ist in Abbildung (TODO ABBILDUNG) dargestellt und zeigt fünf Funktionen, von welchen lediglich die \emph{evaluate()} Funktion immer benötigt wird. Die restlichen vier Funktionen sind optional und können je nach Optimierungsproblem zusätzlich eingesetzt werden. Im Folgenden wird auf die Bedeutung von diesen genauer eingegangen.
\\\\
Die \emph{initialization()} Funktion kann zum Initialisieren der Umgebung verwendet werden und wird einmalig zu Beginn durch die später vorgestellte Bibliothek aufgerufen. Dies kann beispielsweise verwendet werden, um notwendige Daten initial aus einer Datenbank zu laden oder um die Umgebung zu erstellen. Ist die Initialisierung abgeschlossen, können prinzipiell verschiedene Agenten in dieser evaluiert werden. Vor jeder Evaluation wird die \emph{ before\_evaluation()} Funktion aufgerufen, mit welcher zum Beispiel der Zustand der Umgebung zurückgesetzt werden kann, sodass jeder Agent denselben Startzustand vorfindet. Danach wird die eigentliche Evaluation mit der Funktion \emph{evaluate()} durchgeführt. Dies ist die wichtigste Funktion in diesem Interface und muss von jedem Optimierungsproblem implementiert werden. Als Parameter wird ein initialisiertes \ac{KNN} übergeben, welches mit dem Genom eines Agenten erstellt wurde. Mit diesem soll das Optimierungsproblem gelöst werden. Die genaue Implementierung dieser Funktion kann je nach Problem sehr unterschiedlich sein. Verschiedene Beispiele werden in Kapitel \ref{chap:analysis} genauer vorgestellt. Bei der Implementierung ist zu beachten, dass die Funktion zwei Rückgabewerte erwartet. Der erste ist vom Typ \emph{float} und repräsentiert den erreichten Fitnesswert. Wie in Kapitel \ref{subsub:optimzation_problem} vorgestellt, bewertet dieser die Leistung des \ac{KNN} und wird für die spätere Selektionsphase benötigt. Der zweite Rückgabewert ist vom Typ \emph{Dict} und kann verschiedene \emph{Key-Value} Paare mit Zusatzinformationen des Optimierungsproblems enthalten. Diese werden letztendlich im Datenfeld \emph{additional\_info} des Agenten gespeichert, welches in Kapitel \ref{subsubsec:data_structures} vorgestellt ist. Die darin gespeicherten Daten können für eine spätere Auswertung sowie in der Abbruchbedingung für den Algorithmus verwendet werden. Nach jeder durchgeführten Evaluation wird das Pendant zur \emph{before\_evaluation()} Funktion ausführt, dies ist die \emph{after\_evaluation()} Funktion. Mit dieser sowie der letzten in diesem Interface enthaltenen Funktion \emph{clean\_up()} können verschiedene Tätigkeiten umgesetzt werden, welche Ressourcen freigeben oder den originalen Zustand der Umgebung wiederherstellten. Der Unterschied zwischen diesen Funktionen ist, dass die \emph{clean\_up()} Funktion das Pendant zur \emph{initialization()} Funktion ist und nur einmalig am Ende des Trainingsverfahren aufgerufen wird. 
 
\subsubsection{Schnittstelle der Bibliothek}
\label{subsubsec:library_interface}
Nachdem die für diese Arbeit benötigten Komponenten vorgestellt sind, kann auf die Schnittstelle der Bibliothek genauer eingegangen werden. Ziel ist, dass diese die grundlegenden Funktionen definiert, welche sowohl von der sequentiellen als auch der parallelisierten Implementierung umgesetzt werden. Somit bieten beide Implementierungen dieselben Funktionalitäten, können einfach ausgetauscht werden und ermöglichen einen einfachen Vergleich. 
\\\\
Die Schnittstelle der Bibliothek besteht aus drei Interfaces, die als \emph{NeatOptimizer}, \emph{NeatReporter} und \emph{NeatOptimizerCallback} bezeichnet werden und deren Beziehung zueinander in Abbildung (TODO ABBILDUNG) dargestellt ist. Zu erkennen ist, dass der \emph{NeatOptimizerCallback} von dem \emph{NeatReporter} erbt und somit dessen Funktionalität erweitert. Der \emph{NeatOptimizer} besitzt genau einen \emph{NeatOptimizerCallback} sowie beliebig viele \emph{NeatReporter}. Auf die Funktionalität von diesen wird später genauer eingegangen. Im Folgenden wird zuerst der \emph{NeatOptimizer} betrachtet, welcher fünf Funktionen besitzt, von denen vier für das Hinzufügen und Entfernen von Instanzen der Klasse \emph{NeatReporter} und \emph{NeatOptimizerCallback} verwendet werden. Die letzte Funktion hat den Namen \emph{evaluate()}, startet den Ablauf des Optimierungsproblems und erhält als Parameter die hierfür benötigten Werte. Die ersten beiden Parameter bestimmen die Anzahl der \emph{Input}- und \emph{Output}-Neuronen und somit die Größe des Eingabe- und Ausgabevektors. Da der Optimierungsvorgang in \ac{NEAT}, wie in Kapitel \ref{subsec:neat_minimal_structure}  beschrieben, mit einer minimalen Struktur beginnt, müssen keine \emph{Hidden}-Neuronen angegeben werden. Der nächste Parameter ist eine Referenz auf eine Aktivierungsfunktion, welche von allen Neuronen verwendet wird. Einige bekannte Vertreter, von denen ein Teil in Kapitel \ref{subsubsec:activatoin_function} vorgestellt ist, sind standardmäßig in diesem Projekt enthalten, wobei das Hinzufügen von weiteren Funktionen jederzeit möglich ist. Hierauf folgt der Parameter \emph{challenge}, der eine Klasse repräsentiert, die das Interface des Optimierungsproblems implementiert. Der Algorithmus wird versuchen, das hierin enthaltene Problem zu optimieren. Die letzten beiden Parameter sind als \emph{config} und \emph{seed} bezeichnet. Letzteres soll die Generierung der Zufallswerte beeinflussen und somit den Optimierungsvorgang wiederholbar und vergleichbar machen. Die \emph{config} repräsentiert eine Konfiguration, in welcher verschiedene Parameter des Verfahrens spezifiziert sind. In dieser wird beispielsweise angegeben, wie hoch die Chance auf eine strukturelle Mutation ist oder wie sehr sich die Gewichte der Verbindungen ändern können. Auf die tatsächlich verwendeten Konfigurationen wird im Rahmen der Analyse in Kapitel \ref{chap:analysis} weiter eingegangen.
\\\\
Das Ausführen der \emph{evaluate()} Funktion startet den gesamten Optimierungsprozess, welcher je nach Komplexität eine Laufzeit von mehreren Stunden oder Tagen haben kann. Häufig ist es in diesen Anwendungsfällen gewünscht, Zwischenergebnisse und Fortschritte anzuzeigen, sodass die verbleibende Laufzeit und der Erfolg des Verfahrens besser abschätzbar ist. Dies wird in dieser Arbeit durch Callbacks realisiert, welche durch die Interfaces \emph{NeatReporter} und \emph{NeatOptimizerCallback} implementiert werden. Das Interface \emph{NeatReporter} definiert einige Methoden, welche während der Laufzeit des Algorithmus regelmäßig an bestimmen Punkten aufgerufen werden. Klassen, die diese implementieren, können hierdurch Statusinformationen über den aktuellen Zustand sowie den Fortschritt des Verfahrens erhalten. 
Dieses Interface wird in dieser Arbeit unter anderem für die regelmäßige Speicherung des besten Agenten und zur Messung der Performance genutzt. Die erste hierbei implementierte Funktion ist die \emph{on\_initialization()}, welche einmalig zu Beginn aufgerufen wird. Das Pendant hierzu ist die \emph{on\_cleanup()} Funktion, welche einmalig am Ende aufgerufen wird. Auch für die Zwischenzeit gibt es einige Methoden, die den Beginn und das Ende verschiedener Phasen signalisieren. Die Funktionen \emph{on\_generation\_evaluation\_start()} und \emph{on\_generation\_evaluation\_end()} werden zum Beginn und Ende der Evaluationsphase aufgerufen. Als Parameter wird an beide Funktionen die aktuell evaluierte Generation übergeben. Dies kann beispielsweise zum Nachverfolgen des besten und durchschnittlichen Fitnesswertes verwendet werden. Mit den Funktionen \emph{on\_agent\_evaluation\_start()} und \emph{on\_agent\_evaluation\_endt()} wird angegeben, wann die Evaluierung eines einzelnen Agenten beginnt und abgeschlossen ist. An diese Funktion wird der eigentliche Agent sowie ein Index übergeben, welcher angibt, wie viele Evaluationen in dieser Generation bereits durchgeführt wurden. Diese Informationen können beispielsweise für einen Fortschrittsbalken verwendet werden. Die Funktionen \emph{on\_reproduction\_start()} und \emph{on\_reproduction\_end()} markieren den Beginn und das Ende der Reproduktionsphase, welche mit der Selektion startet und mit dem Ersetzen der vorherigen Generation durch die neu erstellten Agenten endet. Um die Zeit zu messen, welche für die Rekombination und Mutation eines einzelnen neuen Agenten benötigt wird, sind die Funktionen \emph{on\_compose\_offsprings\_start()} und  \emph{on\_compose\_offsprings\_end()} enthalten. Die letzte in diesem Interface enthaltene Funktion heißt \emph{on\_finish()} und erhält als Parameter die aktuelle Generation. Sie wird einmalig am Ende der Optimierung aufgerufen nachdem die Abbruchbedingung erfüllt ist. Diese kann verwendet werden, um das beste \ac{KNN} und die erhaltenen Ergebnisse zu visualisieren und zu speichern.
\\\\
Die Klasse \emph{NeatOptimizerCallback} erbt von dem Interface \emph{NeatReporter} und kann daher alle bereits vorgestellten Funktionen nutzen. Diese sind für den Programmablauf optional und werden nicht zwingend benötigt. Dies trifft nicht auf die letzte Funktion zu, welche im \emph{NeatOptimizerCallback} zusätzlich implementiert wird. Dies ist die Funktion \emph{finish\_evaluation()}, welche die aktuelle Generation als Parameter übergeben bekommt und einen Wert vom Typ \emph{boolean} zurückgeben muss. Hiermit wird die Abbruchbedingung umgesetzt. Die Funktion wird nach Beendigung der Evaluationsphase mit der aktuellen Generation sowie mit allen erzielten Fitnesswerten aufgerufen. Ist das Ergebnis der Funktion \emph{True}, wird die Ausführung des Algorithmus beendet und die entsprechenden Callback Funktionen \emph{on\_finish()} und \emph{on\_cleanup()} aufgerufen. Liefert die Funktion \emph{False}, wird mit der Selektion, Rekombination und Mutation fortgefahren und der Zyklus startet erneut. Diese Art der Abbruchbedingung ermöglicht vielfältige Umsetzungen. Zum Beispiel kann der Algorithmus beendet werden, wenn der Fitnesswert eines Agenten einen Schwellwert übersteigt, eine gewisse Anzahl an Zyklen bzw. Generationen durchgeführt ist oder eine festgelegte Trainingszeit überschritten ist.

\subsubsection{Serviceklassen}
Ein wichtiges Ziel bei der Entwicklung ist das Einsparen von unnötigem Implementierungsaufwand, dies gilt auch für diese Arbeit. Aus diesem Grund wurden die bereits vorgestellten Interfaces erstellt, welche sowohl von der sequenziellen als auch der parallelisierten Implementierung verwendet werden. Der Vorteil hierdurch ist, dass ein einfaches Austauschen der beiden Umsetzungen möglich ist und einen einfachen Vergleich ermöglicht. Die eigentliche Implementierung wird sich zwangsläufig an einigen Punkten unterscheiden. Dennoch wird ein großer Teil der \ac{NEAT} Komponenten gleich bleiben, da sich die Funktionen nicht durch eine Parallelisierung ändern. Aus diesem Grund wird ein Großteil dieser Funktionen als Serviceklassen implementiert, die keinen internen Zustand besitzen und somit zwei Vorteile bieten. Erstens ist es einfach möglich, die Funktionen in verschiedenen Implementierungen zu nutzen. Zweitens ermöglicht eine solche Struktur ein einfaches automatisiertes Testen der Implementierung. Dies ist besonders wichtig, da Fehler im Trainingsverfahren zu einem späteren Zeitpunkt aufgrund der Größe der Population schwer zu lokalisieren sind.
\\\\
Insgesamt werden drei Serviceklassen mit den Namen \emph{GenerationService}, \emph{ReproductionService} und \emph{SpeciesService} erstellt, welche Funktionen entsprechend ihrer Benennung übernehmen. Somit befasst sich die erste Klasse mit allen Funktionen, welche die ganze Generation betreffen, die zweite mit Funktionen bezüglich dem Erstellen und Modifizieren von Genomen sowie Agenten und die letzte Klasse mit Funktionen bezüglich der verschiedenen Spezies. Im Folgenden wird auf die enthaltenen Funktionen jeder Serviceklasse genauer eingegangen. Allerdings sei hierzu angemerkt, dass keine genauen Implementierungsdetails vorgestellt werden. Die theoretische Funktionalität ist in den Kapiteln \ref{sec:evolutionary_algos} und  \ref{sec:neat} erläutert. Für die genaue praktische Umsetzung wird auf den veröffentlichten Programmcode verwiesen.
\\\\
Der \emph{ReproductionService} besitzt vor allem Funktionen, um neue Genome mittels Reproduktion zu erzeugen oder bestehende zu mutieren. Die erste Funktion heißt \emph{cross\_over()} und setzt die eigentliche Reproduktion um. In ihr werden zwei Elterngenome verwendet, um einen Nachkommen zu erzeugen. Grundsätzlich entsprechen sowohl die Umsetzungen dieser Funktion als auch der anderen in diesem Kapitel vorgestellten Funktionen den Erläuterungen der vorherigen Kapitel. Eine Besonderheit soll an dieser Stelle hervorgehoben werden. Ein wichtiges Ziel für ein wiederholbares Ergebnis ist, dass unabhängig von einem Prozessor mit demselben Seed immer dasselbe Ergebnis erzielt wird. Dies ist mit einem einfachen globalen Zufallsgenerator in einem verteilten System nicht möglich. Aus diesem Grund wird für jedes Genom ein neuer Zufallsgenerator erzeugt, dessen Seed sich aus den beiden Elterngenomen ergibt und welcher für alle Zufallsoperationen verwendet wird, die dieses Genom betreffen. Der hieraus resultierende Vorteil ist, dass unabhängig vom Prozessor und dessen internem Zustand dieselben Eltern immer denselben Nachkommen produzieren. Der hierfür erstellte Zufallsgenerator wird auch für die Funktionen \emph{mutate\_weights()}, \emph{mutate\_add\_connection()} und \emph{mutate\_add\_node()} verwendet. Die erste Funktion mutiert die Gewichte aller Verbindungen, die zweite fügt eine neue Verbindung und die dritte ein neues Neuron dem Genom hinzu. Bei letzterem werden entsprechend der Definition in Kapitel \ref{subsec:neat_mutation} zusätzlich zwei neue Verbindungen erstellt.
\\\\
Die Klasse \emph{GenerationService} besitzt einige Funktionen zum Erzeugen der initialen Population. Wie bei der Schnittstellendefinition beschrieben, wird anfänglich nur die Anzahl an \emph{Input}- und \emph{Output}-Neuronen übergeben. Die Funktion \emph{create\_genome\_structure()} erzeugt mit diesen Informationen eine Struktur für das \ac{KNN}, welche die entsprechenden Neuronen und Verbindungen enthält. In der Funktion \emph{create\_initial\_generation()} kann mit der generierten Struktur letztendlich eine neue Generation erstellt werden. Hierfür wird das Genom für jeden zu erstellenden Agenten einmal kopiert und die Gewichte der Verbindungen zufällig gesetzt. Schlussendlich werden die Agenten noch den Spezies zugeordnet.
\\\\
Der \emph{SpeciesService} besitzt im Vergleich zum \emph{GenerationService} bedeutend mehr Funktionen. Um die erstellten Agenten den verschiedenen Spezies zuzuordnen, kann die Funktion \emph{ sort\_agents\_into\_species()} genutzt werden. Entsprechend der Erläuterung in Kapitel \ref{subsec:neat_species} iteriert diese über die Liste mit allen Agenten und überprüft für jeden die Kompatibilität mit den existierenden Spezies. Der Kompatibilitätswert zwischen zwei Genomen kann mit der Funktion \emph{calculate\_genetic\_distance()} berechnet werden. Ist dieser kleiner als ein konfigurierter Schwellwert, wird der Agent der Spezies zugeordnet. Existiert keine passende Spezies, wird eine neue erstellt. Dies sind aber nicht die einzigen Funktionen. In Kapitel \ref{subsec:neat_species} ist das \emph{explicit fitness sharing} Verfahren eingeführt worden, welches mit der Funktion \emph{calculate\_adjusted\_fitness()} umgesetzt ist. Der angepasste Fitnesswert wird schließlich bei der Selektion verwendet, welche mit den Funktionen \emph{calculate\_amount\_offspring()} und \emph{create\_offspring\_pairs()} umgesetzt ist. Die erste Funktion berechnet, wie viele Nachkommen jede Spezies erhalten soll. Die zweite Funktion erstellt die tatsächlichen Elternpaare, welche zum Erzeugen der Nachkommen verwendet werden. Zuletzt sind noch einige kleinere Funktionen enthalten. Bei \ac{NEAT} können nur die besten $50\%$ der Genome zur Reproduktion ausgewählt werden, die Funktion \emph{remove\_low\_genomes()} entfernt die restlichen. Beim \ac{NEAT} Algorithmus wird nach jeder Generation ein neues Genom zum Repräsentieren jeder Spezies ausgewählt.  Dies ist in diesem Projekt mit der Funktion \emph{select\_new\_representative()} möglich. Die letzten beiden Funktionen heißen \emph{reset\_species()} und \emph{get\_species\_with\_members()}. Die erste hiervon entfernt alle Mitglieder einer Spezies. Dies wird typischerweise durchgeführt bevor die neu erstellten Agenten den Spezies zugewiesen werden. Nach der Zuweisung wird die zweite Funktion aufgerufen. Diese filtert Spezies heraus, welche keine Mitglieder zugewiesen bekommen haben und entfernt diese aus der Liste. 

\subsubsection{Performance Messung}
Es gibt verschiedene Kriterien um die Performance eines neuroevolutionären Algorithmus zu beurteilen. In der Literatur, so auch in Quelle \cite{stanley2002evolving}, wird häufig die Anzahl an Generationen in Kombination mit der Populationsgröße angegeben bis eine Lösung für ein Optimierungsproblem gefunden wurde. 
Hiermit kann die Anzahl der evaluierten \ac{KNN} abgeleitet werden und ein Vergleich zu anderen Verfahren ist möglich. In vielen Fällen ist dies sinnvoller, als ein direkter Vergleich der tatsächlichen Laufzeiten, da diese sehr von der verwendeten Hardware, Programmiersprache und der effizienten Implementierung abhängig sind beziehungsweise beeinflusst werden können. Auch in dieser Arbeit wird diese Art der Performance Messung mit der Klasse \emph{FitnessReporter} durchgeführt, welcher zwei Aufgaben erfüllt. Diese erbt von der bereits vorgestellten Klasse \emph{NeatReporter} und hat somit Zugriff auf die Funktion \emph{on\_generation\_evaluation\_end()}. Der \emph{FitnessReporter} legt für jede Generation einen Datensatz an, in welcher der durchschnittliche und beste Fitnesswert gespeichert werden. Hierdurch ist einerseits ersichtlich, wie viele Generationen der Algorithmus benötigt und zusätzlich kann der Fortschritt zwischen den einzelnen Generationen betrachtet werden. Dies kann wertvolle Erkenntnisse liefern, wenn der Algorithmus schlechter oder langsamer ist als erwartet. 
\\\\
In dieser Arbeit spielt auch die tatsächlich benötigte Ausführungszeit eines Algorithmus, welche auch als \emph{wall clock time} bezeichnet wird, eine große Rolle. Im Rahmen dieser Arbeit werden die Zeiten erfasst, und dienen als Basis für den Vergleich zwischen der sequenziellen und parallelisierten Implementierung. Hierfür wird die Klasse \emph{TimeReporter} erstellt, welche ebenfalls das Interface \emph{NeatReporter} implementiert und Zugriff auf die verschiedenen bereits vorgestellten Funktionen besitzt. Mit diesen können die benötigten Ausführungszeiten für verschiedene Phasen des Algorithmus erfasst werden. Auf die genaue Unterteilung wird im Rahmen von Kapitel \ref{chap:analysis} genauer eingegangen. 

\subsubsection{Visualisierung}
Die verschiedenen erfassten Messwerte sollen für eine bessere und einfachere Auswertung graphisch dargestellt werden können. Die beste Fitnesswerte pro Generation soll in einem Liniendiagramm und die erfassten Ausführungszeiten in einem Säulendiagramm dargestellt werden. Bei letzterem sollen die verschiedenen Phasen gestapelt sein. Dies hat den Vorteil, dass sowohl das Verhältnis der verschiedenen Phasen zueinander als auch die gesamte Ausführungszeit über mehrere Generationen hinweg ausgewertet werden kann. Für diese Diagramme soll das Paket Matplotlib für die Sprache Python verwendet werden \cite{pyplot2007hunter}. Dieses ermöglicht ein einfaches und schnelles erstellen von diversen Diagrammtypen und ist zusätzlich sehr gut in die Entwicklungsumgebung Pycharm integriert, welche im Rahmen dieser Arbeit verwendet wird.
\\\\
Zusätzlich zu den Diagrammen sollen auch die erstellten \ac{KNN} visualisiert werden, was eine größere Herausforderung darstellt. Eine mögliche Umsetzung welche in diesem Rahmen evaluiert wird, nutzt die Software Grapviz \cite{graphviz2000gansner}. Diese besitzt eine eigene Beschreibungssprache, welche als \emph{DOT} bezeichnet wird und die Beschreibung von verschiedene Graphentypen ermöglicht. Würde ein \ac{KNN} in dieser Arbeit mit Graphviz visualisiert werden, müssten die im  Genom kodierten Informationen  im \emph{DOT} Format in eine Textdatei exportiert werden und erst danach kann ein PDF oder ähnliches mithilfe von Graphviz erstellt werden. Da dieser Vorgang relativ aufwändig ist, wird eine alternative Lösung mithilfe des Pythonpakets NetworkX umgesetzt \cite{networkx2008hagberg}. Dieses wird normalerweise primär für die Analyse von Graphen eingesetzt, aber ermöglicht auch die Visualisierung von diesen. Als Basis hierfür verwendet das Paket entweder die vorgestellte Software Graphviz oder alternativ Matplotlib. 
letzteres bietet sich mehr an, da es bereits in diesem Projekt verwendet wird und eine bessere Integration in die Entwicklungsumgebung bietet. Allerdings müssen auch bei diesem Verfahren die im Genom enthaltenen Informationen in einem gewissen Format an das Paket übergeben werden. Eine weitere Herausforderung bei der Implementierung ist, dass die Knoten des Graphen, in diesem Fall die Neuronen, normalerweise eine zufällige Position zugewiesen bekommen und daher keine Darstellung der Schichten möglich ist. Um diese zu erhalten müssen sämtliche Positionen manuell gesetzt werden. Hierbei ist das Bestimmen der X-Koordinate einfach, da diese für jedes Neuron bereits im Genom gespeichert ist. Die Y-Koordinate wird dann in Abhängigkeit von der Anzahl an weiteren Neuronen in derselben Schicht bestimmt.    

\subsubsection{Persistenz}
Die persistente Speicherung der Optimierungsergebnisse ist die letzte Anforderung für diese Arbeit. Dies umfasst sowohl die Speicherung der Genome sowie die Ergebnisse der Performance Messung. Für die Speicherung der Daten wurden prinzipiell zwei Ansätze evaluiert. Beim ersten Ansatz würde eine klassische SQL- oder NoSQL-Datenbank zur Speicherung eingesetzt werden. Der Vorteil von einem solchen System ist, dass viele Nutzer gleichzeitig die Daten lesen können. Allerdings ist dies für den gegebenen Anwendungskontext nicht notwendig und der Aufwand, welcher durch die Implementierung der verschiedenen Anfragen entsteht, ist im Vergleich zur zweiten Variante bedeutend höher. Zusätzlich muss ein dauerhaft verfügbarer Datenbankservice zur Verfügung gestellt werden, sodass die Ergebnisse mit anderen Nutzern teilbar sind.
\\\\
Der zweite Ansatz ist bedeutend einfacher. Das Ziel ist, dass die Ergebnisse nur in einer lokalen Datei gespeichert beziehungsweise aus einer solchen Datei geladen werden können. Ein solches Vorgehen bietet einige Vorteile. Der erste ist, dass ein solches Verfahren mit wenig Aufwand implementierbar ist. Das standardmäßig in Python enthaltene Pickle Modul erfüllt genau diese Voraussetzungen. Dieses bietet sowohl vorgefertigte Funktionen zum serialisieren von unterschiedliche Python Objekte welche dann in einer Datei gespeichert werden können sowie Funktionen zum Laden von solchen Datensätzen. Zwei Vorteile von diesem Verfahren sind, dass der programmiertechnische Aufwand sehr gering ist und dass trainierte Modelle als Datei vorliegen wodurch sie auf Github oder ähnlichem einfach veröffentlicht werden können.
\\\\
Das eigentliche Speichern soll auf jeden Fall am Ende das Trainingsverfahrens durchgeführt werden. Allerdings kann es zusätzlich sinnvoll sein, in regelmäßigen Abständen die bis dahin erhaltenen Zwischenergebnisse ebenfalls zu sichern. Hierfür gibt es verschiedene Gründe. Bei einer ungünstigen Konfiguration kann ein Trainingsverfahren eventuell niemals die Abbruchbedingung erfüllen. In diesem Fall müsste das Programm manuell abgebrochen werden wobei sämtliche Ergebnisse verloren gehen würden. Auch bei Implementierungsfehler können zu einem Absturz führen, bei welchem nicht die finalen Ergebnisse gespeichert werden. Bei der parallelisierten Implementierung können zusätzlich Hardware- und Netzwerkfehler auftreten für die \ac{MPI}, wie in Kapitel \ref{subsubsec:hpc_architecture} beschrieben, anfällig ist und ebenfalls zu einem Absturz der Anwendung führen. Aus diesem Grund wird die Klasse \emph{CheckPointReporter} implementiert, welche ebenfalls das Interface \emph{NeatReporter} umsetzt. Hierbei werden die Funktionen \emph{on\_generation\_evaluation\_end()} und \emph{on\_finish()} implementiert. Bei Aufruf von letzterem wird das Ergebnis immer gespeichert. Bei der \emph{on\_generation\_evaluation\_end()} ist es abhängig von der gewählten Konfiguration. In dieser wird spezifiziert, nach wie vielen Generationen ein Zwischenergebnis abgespeichert werden soll. Prinzipiell ist es möglich sowohl nach jeder Generation die Ergebnisse zu speichern oder gar nicht, wobei letzteres nicht zu empfehlen ist.

\section{Sequenzielle Implementierung}
\label{sec:sequential_implementation}
Im vorherigen Kapitel sind die verschiedenen Komponenten dieser Arbeit vorgestellt, welche zur Implementierung des sequenziellen Verfahrens verwendet werden. Der grundsätzliche Ablauf entspricht hierbei den Erläuterungen der Kapiteln \ref{sec:evolutionary_algos} und \ref{sec:neat}. Daher wird in diesem Kapitel hauptsächlich auf die technische Umsetzung sowie das Zusammenspiel der bereits vorgestellten Komponenten eingegangen. 
\\\\
Die \emph{evaluate()} Funktion ist, wie in Kapitel \ref{subsubsec:library_interface} beschrieben, der Einstiegspunkt für die Bibliothek. Ein Überblick über diese ist im Sequenzdiagramm in Abbildung (TODO Abbildung) gegeben. Initial werden die bereits spezifizierten \emph{NeatReporter} und der \emph{NeatOptimizerCallback} mit der Funktion \emph{on\_initialization()} über den Beginn des Verfahrens informiert. Direkt im Anschluss wird auch das Optimierungsproblem initialisiert und die initiale Generation erstellt. Danach beginnt der Zyklus aus Evaluation, Selektion, Rekombination und Mutation. Hierfür wird zuerst die \emph{evaluate\_generation()} Funktion aufgerufen, deren Implementierung im weiteren Verlauf noch genauer erläutert wird. Grundsätzlich werden die verschiedenen Agenten im Optimierungsproblem evaluiert und erhalten einen Fitnesswert. Nach Abschluss dieser Phase wird überprüft, ob das Verfahren terminiert werden soll, indem die Funktion \emph{finish\_evaluation()} des \emph{NeatOptimizerCallbacks} aufgerufen wird. Ist dies der Fall, wird die Schleife abgebrochen, die finale Generation zurückgegeben und die \emph{NeatReporter} sowie der \emph{NeatOptimizerCallback} über hierüber informiert. Andernfalls wird mit der Funktion \emph{build\_new\_generation()} eine neue Population durch die Phasen Selektion, Rekombination und Mutation erstellt. Auch hierauf wird in diesem Kapitel noch genauer eingegangen. 
\\\\
Der Ablauf der \emph{evaluate\_generation()} Funktion ist genauer in Abbildung (TODO ABBILDUNG) dargestellt. Am Anfang werden die \emph{NeatReporter} und der \emph{NeatOptimizerCallback} über den Beginn der Funktion informiert. Im Anschluss wird über alle Agenten iteriert und für jeden die Evaluation durchgeführt. Dies umfasst das Aufrufen der entsprechenden Funktionen im \emph{NeatOptimizerCallback} und den \emph{NeatReportern}, sowie das Erstellen und Evaluieren \ac{KNN}.
\\\\
Abbildung (TODO Abbildung) zeigt den Ablauf der \emph{build\_new\_generation()} Funktion. In dieser sind die meisten Funktionen von \ac{NEAT} enthalten und dementsprechend aufwendig ist die Implementierung. Zu Beginn werden wie bei der  \emph{evaluate\_generation()} Funktion, die \emph{NeatReporter} und der \emph{NeatOptimizerCallback} über den Beginn dieser Phase informiert. Danach werden entsprechend der originalen Publikation von \ac{NEAT} die besten Genome jeder Spezies mit mehr als $x$ Mitglieder selektiert und unverändert in die nächste Generation kopiert, wobei $x$ ein konfigurierbarer Schwellwert ist. Im Anschluss wird über jede Spezies iteriert und der höchste erreichte Fitnesswert aktualisiert, sofern sich dieser in der letzten Generation geändert hat. Dies ist für den nächsten Schritt wichtig, da bei diesem die Spezies entfernt werden, welche keine Fitnesssteigerung in den letzten $t$ Generationen erzielt haben, wobei auch $t$ konfigurierbar ist. Die Mitglieder von diesen können in der nachfolgenden Selektion nicht als Elternteile ausgewählt werden.
\\\\
Für die verbleibenden Spezies wird im Anschluss der angepasste Fitnesswert berechnet und auf Basis von diesem im darauffolgenden Schritt die Anzahl an Nachkommen bestimmt. Bevor es zur eigentlichen Selektion kommt, werden noch die Agenten entfernt, welche im vorherigen Durchlauf schlechte Fitnesswerte erzielt haben. Diese können nicht für die Reproduktion ausgewählt werden. Nachdem diese Schritte durchgeführt sind, kann die eigentliche Selektion mit der Funktion \emph{create\_offspring\_pairs()} beginnen. Im Rahmen dieser Funktion wird über jede Spezies iteriert und für jeden zu erzeugenden Nachkommen zwei Elternteile zufällig ausgewählt, welche als Paar in einer Liste zwischengespeichert werden. Anhand dieser Liste wird die Rekombination und Mutation durchgeführt. Durch die Funktion \emph{cross\_over()} wird die Rekombination implementiert, welche das Genom für den Nachkommen erzeugt. Anschließend wird das Genom noch mutiert. Hierfür werden die Funktion \emph{mutate\_weights()}, \emph{mutate\_add\_node()} und \emph{mutate\_add\_connection()} nacheinander aufgerufen.
\\\\
Bevor das sortieren neu erstellten Agenten in die Spezies beginnen kann, müssen noch zwei andere Funktionen ausgeführt werden. Zuerst wird mit der \emph{select\_new\_representative()} Funktion für jede Spezies aufgerufen, welche ein Mitglied zufällig auswählt und dessen Genom als Repräsentant setzt. Danach werden durch den Aufruf der Funktion \emph{reset\_species()} alle bisherigen Mitglieder entfernt. Erst an dieser Stelle können die neuen Agenten auf Basis ihres Kompatibilitätswertes tatsächlich den verschiedenen Spezies zugeordnet. Als letzte Aktionen in der \emph{build\_new\_generation()} Funktion werden die Spezies herausgefiltert, welche keine Mitglieder erhalten haben und eine neue Generation mit den Agenten und Spezies erstellt. Diese wird als Ergebnis der Funktion zurückgegeben. 

% TODO Species ID Generator, wenn parallelisierung hiervon angesprochen wird
