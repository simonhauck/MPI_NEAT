% !TeX spellcheck = de_DE
\section{Softwarearchitektur}
Im Folgenden wird auf die Architektur der Software, sowie der Implementierung des sequenziellen Verfahrens eingegangen. Das grundsätzliche Ziel ist, dass so viele Komponenten wie möglich von der sequentiellen und parallelisierten Implementierung genutzt werden können, sodass einerseits der programmiertechnische Aufwand möglichst gering ist und andererseits dass ein einfaches Austauschen der beiden Varianten möglich ist. Hierfür wird eine Kombination aus verschiedenen Interfaces sowie Serviceklassen verwendet auf welche  im Rahmen dieses Kapitels genauer eingegangen wird. Zuvor werden noch die verwendeten Datenstrukturen und das Interface für die Bibliothek vorgestellt.
 
\subsubsection{Datenstrukturen}
\label{subsubsec:data_structures}
Die verwendeten Datenstrukturen sind für beide Implementierung identisch und in Abbildung (TODO ABBILDUNG) in einem UML Diagramm dargestellt. Wie zu erkennen ist, besitzen diese keine Funktionen oder Methoden und können so auf jeden Fall für beide Implementierungen genutzt werden. Die eigentliche Funktionalität wird in den später vorgestellten Serviceklassen umgesetzt.
\\\\
Die erste Struktur ist das \emph{Genom}, welches aus Kapitel \ref{sec:neat} bekannt ist und alle Informationen zur Konstruktion eines \ac{KNN} besitzt. Hierfür werden zwei Listen benötigt, wobei die erste alle Neuronen und die zweite alle Verbindungen des \ac{KNN} enthält. Diese Repräsentation entspricht der Kodierung aus Kapitel \ref{subsec:neat_encoding}. 
\\\\
Ein Neuron wird durch die Klasse \emph{Node} repräsentiert und enthält unter anderem den Bias-Wert und die zu verwendende Aktivierungsfunktion, welche für die Berechnungen im \ac{KNN} benötigt werden. Zusätzlich sind einige weitere Werte vorhanden, welche die spätere Implementierungen vereinfachen. Hierzu gehört unter anderem der Neuronentyp, welcher angibt ob es sich um ein \emph{Input}-, \emph{Hidden}- oder \emph{Ouput}-Neuron handelt. Dies vereinfacht die spätere Visualisierung sowie das Einsetzen der Eingabewerte und das Auslesen des Ausgabevektors. Zusätzlich besitzt jedes Neuron eine X-Koordinate, welche die relative X-Position für eine spätere Visualisierung angibt. Die \emph{Input}- und \emph{Output}-Neuronen besitzen die Werte $0$ und $1$. Die \emph{Hidden}-Neuronen werden, wie in Kapitel \ref{subsec:neat_minimal_structure} beschrieben, nur durch strukturelle Mutationen hinzugefügt und dabei zwischen zwei anderen Neuronen platziert. Die X-Koordinate von diesen wird berechnet, indem der Mittelwert der zwei anderen Neuronen gebildet wird. Zuletzt enthält jedes Neuron zur Identifizierung eine ID, welche von den Verbindungen genutzt wird.
\\\\
Eine Verbindung zwischen zwei Neuronen wird im Genom durch eine Instanz der Klasse \emph{Connection} repräsentiert. Jede von diesen enthält zwei Werte, welche die ID des Start- und Zielneurons der Verbindung enthalten. Zusätzlich ist das Gewicht sowie ein Aktivierungsbit gegeben. Letzteres gibt entsprechend der vorgestellten Kodierung an, ob die Verbindung im \ac{KNN} enthalten sein soll. Die letzte Variable ist die Innovationsnummer, welche in Kapitel \ref{subsec:neat_reproduction} beschrieben ist und durch strukturelle Mutationen zugewiesen, und bei der Reproduktion verwendet wird.
\\\\
In Kapitel \ref{sec:evolutionary_algos} ist der Begriff des Individuums eingeführt worden. In dieser Arbeit werden diese durch Instanzen der Klasse \emph{Agent} repräsentiert und sind nach dem Agenten benannt, welcher beim bestärkenden Lernen mit der Umwelt interagiert. Der hierbei erhaltene Fitnesswert, welcher im späteren Verlauf die Selektion maßgeblich beeinflusst, wird in der entsprechenden Variable \emph{fitness} gespeichert. Das Feld \emph{additional\_info} kann Zusatzinformationen enthalten, welche vom Optimierungsproblem stammen können.
\\\\
Die zwei letzten Klassen in diesem Diagramm sind die \emph{Generation} und \emph{Spezies}. Letzteres ist aus dem \ac{NEAT} Kapitel bekannt und ist eine Gruppierung von ähnlichen Individuen bzw. Agenten. Entsprechend der in Kapitel \ref{subsec:neat_species} enthaltenen Definition, ist eine Spezies durch ein Genom repräsentiert und besitzt eine Mitgliederliste. Zusätzlich wird noch der höchste erreichte Fitnesswert von dem besten Mitglied gespeichert, sowie die Generation, in welcher dieser Wert erzielt wurde. Dies wird benötigt, um den Fortschritt zu überwachen, da eine Spezies, bei welcher die Mitglieder keine Steigerung des Fitnesswertes in einer festgelegten Anzahl an Generationen erzielen, nicht für die Reproduktion ausgewählt wird.
\\\\
Die letzte Klasse ist die \emph{Generation}, welche aus Kapitel \ref{subsec:evolutionary_algorithm} bekannt ist. Diese enthält sowohl eine Liste mit allen Individuen als auch der verschiedenen Spezies. Zusätzlich wird diese durch eine Nummer identifiziert, welche angibt wie viele Zyklus aus Evaluation, Selektion, Rekombination und Mutation bereits durchgeführt wurden.

\subsubsection{Künstliches neuronales Netz}
Aus dem Genom wird ein \ac{KNN} erstellt, welches dann im Optimierungsproblem eingesetzt wird. In Kapitel \ref{sec:neuroal_networks} sind der Aufbau und die Funktionsweise von diesen ausführlich erläutert. Prinzipiell können hierfür verschiedene bereits implementierte Bibliotheken verwendet werden, wie beispielsweise Tensorflow \cite{tensorflow2015} oder Pytorch \cite{pytorch2019}. Mit diesen können vor allem rein ebenenweise verbundene \ac{KNN}, wie sie in Kapitel \ref{subsec:network_structures} vorgestellt sind, schnell und einfach erstellt werden. Allerdings können durch \ac{NEAT} auch \ac{KNN} mit \emph{shortcut} Verbindungen entstehen sowie Netze mit Rückkopplungen. Zwar können diese  auch mit Tensorflow und Pytorch abgebildet werden aber dies erfordert einen bedeutend höheren Aufwand. Da dies nicht im Rahmen dieser Arbeit möglich ist wird eine eigene vereinfachte Implementierung verwendet. Ein Nachteil von dieser ist, dass sie im Vergleich zu Implementierungen aus große Bibliotheken insgesamt langsamer ist. Der Grund hierfür ist, dass die Bibliotheken gut optimiert sind und auch die Verwendung von \acp{GPU} unterstützen, welche die benötigte Ausführungszeit stark reduzieren können. Auch wenn diese nicht verwendet werden, wird im Folgenden ein Grundgerüst erstellt, sodass eine spätere Integration einfach möglich ist. Dies wird mit einem Interface realisiert, welches die grundlegenden Funktionen definiert. Sowohl die in dieser Arbeit verwendete Implementierung, als auch zukünftige Erweiterungen mit Tensorflow oder ähnlichem können dieses nutzen und ermöglichen somit ein einfaches Austauschen der verschiedenen Implementierungen. Das Interface ist in Abbildung (TODO ABBILDUNG) dargestellt und definiert nur drei Funktionen mit den Namen \emph{build()}, \emph{reset()} und \emph{activate()}. Bei der \emph{build()} Funktion wird ein Genom übergeben und hieraus ein \ac{KNN} konstruiert. Die \emph{activate()} Funktion bekommt eine Liste mit Eingabewerte übergeben und produziert den Ausgabevektor des \ac{KNN}. Die Anzahl an Eingabe- und Ausgabewerten ist entsprechend der Anzahl an \emph{Input}- und \emph{Output}-Neuronen im \ac{KNN}. Bei der \emph{reset()} Funktion werden eventuell gespeicherte Ergebnisse, wie sie bei einem Netz mit Rückkopplungen enthalten sein können, entfernt und auf den Startwert zurückgesetzt. 
\\\\
Die in dieser Arbeit verwendete \emph{BasicNeuralNetwork} Implementierung setzt dieses Interface um, mit dem Ziel dass sowohl Netze mit und ohne Rückkopplungen umgesetzt werden können. Da eine genaue Beschreibung der Implementierung nicht von Interesse ist, wird an dieser Stelle nur der oberflächliche Ablauf beschrieben. Bei der \emph{build()} Funktion wird je eine Liste für die \emph{Input}- und \emph{Output}-Neuronen angelegt, in welche die Neuronen mit den entsprechenden Typen sortiert werden. Eine dritte Liste enthält alle im \ac{KNN} enthaltenen Neuronen. Jedes Neuron speichert zudem eine Liste mit den Verbindungen, welche zu ihm führen und den aktuellen sowie letzten berechneten Wert. Die letzte wichtige Aufgabe dieser Funktion ist, die Reihenfolge festzulegen, in welcher die einzelnen Neuronen aktiviert bzw. deren Zwischenergebnisse berechnet werden. Bei der Reihenfolge ist zu beachten, dass bei einem Netz ohne Rückkopplung jedes vorherige Neuron bis zu den \emph{Input-Neuronen} bereits aktiviert sein muss, da ansonsten das Ergebnis verfälscht wird. Bei Netzen mit Rückkopplung gilt diese Anforderung prinzipiell auch, außer für Verbindungen welche in derselben Schicht oder von Neuronen der nachfolgenden Schichten ausgehen. Dies sind Rückkopplungen welche den zuletzt berechneten Wert zurückgeben, sodass keine Endlosschleife entsteht. 
\\\\
Bei der \emph{activate()} Funktion wird ein Eingabevektor an das \ac{KNN} übergeben. Jeder darin enthaltene Wert wird in ein \emph{Input}-Neuron gesetzt. Dies ist mit der zuvor in der \emph{build()} Funktion erstellten Liste mit allen \emph{Input}-Neuronen effizient umgesetzten. Danach beginnt die eigentliche Berechnung des Ausgabevektors. Hierzu werden die Neuronen in der zuvor festgelegten Reihenfolge aktiviert bzw. berechnet. Zuletzt wird noch der Ausgabevektor erstellt. 
Auch dies ist effizient umsetzbar, da zuvor ein Liste mit allen \emph{Output}-Neuronen erstellt wurde. Über  diese wird iteriert und die entsprechenden Ergebnisse in eine neue Liste kopiert, welche schlussendlich den Ausgabevektor repräsentiert und als Ergebnis der Funktion zurück gegeben wird. 
\\\\
Die \emph{reset()} Funktion iteriert über die Liste mit allen Neuronen und entfernt die gespeicherten Zwischenergebnisse. 

\subsubsection{Optimierungsproblem}
In den Kapiteln zuvor sind die grundlegenden Datenstrukturen und die Funktionalität des \ac{KNN} vorgestellt. Bevor der eigentliche Optimierungsalgorithmus implementiert werden kann wird noch eine grundlegende Komponente benötigt, das Optimierungsproblem. Wie in Kapitel \ref{subsub:optimzation_problem} vorgestellt aus verschiedenen Domänen stammen und sich sehr unterscheiden. Somit muss auch für dieses ein Interface erstellt werden, mit dem Ziel, dass möglichst viele Szenarien abgebildet werden können, welche dann einfach und mit wenig Aufwand durch das in dieser Arbeit implementierte Verfahren optimierbar sind.
\\\\
Das Interface ist in Abbildung (TODO ABBILDUNG) dargestellt und zeigt fünf Funktionen, von welchen nur die \emph{evaluate()} Funktion immer benötigt wird. Die restlichen vier sind optional und können je nach Optimierungsproblem zusätzlich eingesetzt werden. Im Folgenden wird auf die Bedeutung von diesen genauer eingegangen.
\\\\
Die \emph{initialization()} Funktion kann zum Initialisieren der Umgebung verwendet werden und wird einmalig zu Beginn durch die später vorgestellte Bibliothek aufgerufen. Dies kann beispielsweise verwendet werden um notwendige Daten initial aus einer Datenbank zu laden oder um die Umgebung zu erstellen. Ist die Initialisierung abgeschlossen können prinzipiell verschiedene Agenten in dieser evaluiert werden. Vor jeder Evaluation wird die \emph{ before\_evaluation()} Funktion aufgerufen, mit welcher zum Beispiel der Zustand der Umgebung zurückgesetzt werden kann, sodass jeder Agent denselben Startzustand vorfindet. Hiernach wird die eigentliche Evaluation mit der Funktion \emph{evaluate()} durchgeführt. Dies ist die wichtigste Funktion in diesem Interface, muss von jedem Optimierungsproblem implementiert werden und wird wie die anderen Funktionen auch durch die Bibliothek aufgerufen. Hierbei wird als Parameter ein initialisiertes \ac{KNN} übergeben, welches mit dem Genom eines Agenten erstellt wurde. Mit diesem soll das Optimierungsproblem gelöst werden. Die genaue Implementierung dieser Funktion kann je nach Problem sehr unterschiedlich sein. Verschiedene Beispiele werden in Kapitel \ref{chap:analysis} genauer vorgestellt. Bei der Implementierung ist zu beachten, dass die Funktion zwei Rückgabewerte erwartet. Der erste ist vom Typ \emph{float} und repräsentiert den erreichten Fitnesswert. Wie in Kapitel \ref{subsub:optimzation_problem} vorgestellt, bewertet dieser wie gut oder schlecht die Leistung des \ac{KNN} ist und wird für die spätere Selektionsphase benötigt. Der zweite Rückgabewert ist vom Typ \emph{Dict} und kann verschiedene \emph{Key-Value} Paare mit Zusatzinformationen des Optimierungsproblems enthalten. Diese werden letztendlich im Datenfeld \emph{additional\_info} des Agenten gespeichert, welches in Kapitel \ref{subsubsec:data_structures} vorgestellt ist. Die darin gespeicherten Daten können für eine spätere Auswertung sowie in der Abbruchbedingung für den Algorithmus verwendet werden. Nach jeder durchgeführten Evaluation wird das Pendant zur \emph{ before\_evaluation()} Funktion ausführt, die \emph{after\_evaluation()} Funktion. Mit dieser, sowie der letzten in diesem Interface enthaltene Funktion \emph{clean\_up()}, können verschiedene Tätigkeiten umgesetzt werden, welche Ressourcen freigeben oder den originalen Zustand der Umgebung wiederherstellten. Der Unterschied zwischen diesen Funktionen ist, dass die \emph{clean\_up()} Funktion das Pendant zur \emph{initialization()} Funktion ist und nur einmalig am Ende des Trainingsverfahren aufgerufen wird. 
 
\subsubsection{Schnittstelle der Bibliothek}
Nachdem die für diese Arbeit benötigten Komponenten vorgestellt sind, kann auf die die Schnittstelle der Bibliothek genauer eingegangen werden. Ziel ist, dass dieses die grundlegenden Funktionen definiert, welche sowohl von der sequentiellen als auch parallelisierten Implementierung umgesetzt werden. Somit bieten beide Implementierungen dieselbe Funktionalitäten, können einfach ausgetauscht werden und ermöglichen einen einfachen Vergleich. 
\\\\
Die Schnittstelle der Bibliothek besteht aus drei Interfaces, die als \emph{NeatOptimizer}, \emph{NeatReporter} und \emph{NeatOptimizerCallback} bezeichnet werden und deren Beziehung zueinander in Abbildung (TODO ABBILDUNG) dargestellt ist. Uu erkennen ist, dass der \emph{NeatOptimizerCallback} von dem \emph{NeatReporter} erbt und somit dessen Funktionalität erweitert. Der \emph{NeatOptimizer} besitzt genau einen \emph{NeatOptimizerCallback} sowie beliebig viele \emph{NeatReporter}. Auf die genaue Funktionalität von diesen wird später genauer eingegangen. Im Folgenden wird zuerst der \emph{NeatOptimizer} betrachtet, welcher fünf Funktionen besitzt, von welchen vier für das Hinzufügen und Entfernen von Instanzen der Klasse \emph{NeatReporter} und \emph{NeatOptimizerCallback} verwendet werden. Die letzte Funktion hat den Namen \emph{evaluate()}, startet den Ablauf des Optimierungsproblems und erhält als Parameter die hierfür nötigen Werte. Die ersten beiden bestimmen die Anzahl der \emph{Input}- und \emph{Output}-Neuronen und somit auch die Größe des Eingabe- und Ausgabevektors. Da der Optimierungsvorgang in \ac{NEAT}, wie in Kapitel \ref{subsec:neat_minimal_structure}  beschrieben, mit einer minimalen Struktur beginnt, müssen keine \emph{Hidden}-Neuronen angegeben werden. Der nächste Parameter ist eine Referenz auf eine Aktivierungsfunktion, welche von allen Neuronen verwendet wird. Einige bekannte Vertreter, von denen ein Teil in Kapitel \ref{subsubsec:activatoin_function} vorgestellt ist, sind standardmäßig in diesem Projekt enthalten, wobei das Hinzufügen von weiteren Funktionen jederzeit möglich ist. Hierauf folgt der Parameter \emph{challenge}, was eine Klasse repräsentiert die das Interface des Optimierungsproblems implementiert. Der Algorithmus wird das hierin enthaltene Probleme versuchen zu optimieren. Die letzten beiden Parameter sind als \emph{config} und \emph{seed} bezeichnet. Letzteres soll die Generierung der Zufallswerte beeinflussen und somit den Optimierungsvorgang wiederholbar und vergleichbar machen. Die \emph{config} repräsentiert eine Konfiguration, in welcher verschiedenste Parameter des Verfahrens spezifiziert sind. In dieser wird beispielsweise angegeben wie hoch die Chance auf eine strukturelle Mutation ist oder wie sehr sich die Gewichte der Verbindungen ändern können. Auf die tatsächlich verwendeten Konfigurationen wird im Rahmen der Analyse in Kapitel \ref{chap:analysis} weiter eingegangen.
\\\\
Das Ausführen der \emph{evaluate()} Funktion startet den ganzen Optimierungsprozess welcher je nach Komplexität eine Laufzeit von mehreren Stunden und Tagen haben kann. Häufig ist es in diesen Anwendungsfällen gewünscht Zwischenergebnisse und Fortschritte anzuzeigen, sodass die verbleibende Laufzeit und auch der Erfolg des Verfahrens besser abschätzbar ist. Dies wird in dieser Arbeit durch einen Callbacks realisiert, welche durch die Interfaces \emph{NeatReporter} und \emph{NeatOptimizerCallback} implementiert werden. Das Interface \emph{NeatReporter} definiert einige Methoden, welche während der Laufzeit des Algorithmus regelmäßig an bestimmen Punkten aufgerufen werden. Klassen, die diese implementieren, können hierdurch Statusinformationen über den aktuellen Stand sowie Fortschritt des Algorithmus erhalten. Dieses Interface wird in dieser Arbeit unter anderem für die regelmäßige Speicherung des besten Agenten und zur Messung der Performance genutzt. Die erste hierbei implementierte Funktion ist die \emph{on\_initialization()}, welch einmalig zu Beginn aufgerufen wird. Hierzu ist das Pendant die \emph{on\_cleanup()} welche einmalig am Ende aufgerufen wird. Auch für die Zwischenzeit gibt es einige Methoden, die den Beginn und das Ende verschiedener Phasen signalisieren. Die Funktionen \emph{on\_generation\_evaluation\_start()} und \emph{on\_generation\_evaluation\_end()} werden zum Beginn und Ende der Evaluationsphase aufgerufen. Als Parameter wird an beide Funktionen die aktuell evaluierte Generation übergeben. Dies kann beispielsweise zum Nachverfolgen des besten und durchschnittlichen Fitnesswertes verwendet werden. Mit den Funktionen \emph{on\_agent\_evaluation\_start()} und \emph{on\_agent\_evaluation\_start()} wird angegeben, wann die Evaluierung eines einzelnen Agenten beginnt und abgeschlossen ist. An diese Funktion wird der eigentliche Agent sowie ein Index übergeben, welcher angibt wie viele Evaluationen bereits durchgeführt wurden. Diese Informationen können beispielsweise für einen Fortschrittsbalken verwendet werden. Die Funktionen \emph{on\_reproduction\_start()} und \emph{on\_reproduction\_end()} markieren den Beginn und das Ende der kompletten Reproduktionsphase, welche mit der Selektion startet und mit dem Ersetzen der vorherige Generation durch die neu erstellten Agenten endet. Um die Zeit zu messen, welche für die Rekombination und Mutation für einen einzelnen neuen Agenten benötigt wird, sind die Funktionen \emph{on\_compose\_offsprings\_start()} und  \emph{on\_compose\_offsprings\_end()} enthalten. Die letzte in diesem Interface enthaltene Funktion heißt \emph{on\_finish()} und erhält als Parameter die aktuellen Generation und wird einmalig am Ende der Optimierung aufgerufen nachdem die Abbruchbedingung erfüllt ist. Diese kann verwendet werden um das beste \ac{KNN} und die erhaltenen Ergebnisse zu visualisieren und zu speichern.
\\\\
Die Klasse \emph{NeatOptimizerCallback} erbt von dem Interfaces \emph{NeatReporter} und kann daher auch alle bereits vorgestellten Funktionen nutzen. Diese sind für den Programmablauf optional und werden nicht zwingend benötigt. Dies trifft nicht auf die letzte Funktion zu, welche im \emph{NeatOptimizerCallback} zusätzlich implementiert wird. Dies ist die Funktion \emph{finish\_evaluation()}, welche die aktuelle Generation als Parameter übergeben bekommt und einen Wert vom Typ \emph{boolean} zurückgeben muss. Hiermit wird die Abbruchbedingung umgesetzt. Die Funktion wird nach Beendigung der Evaluationsphase mit der aktuellen Generation sowie mit allen erzielten Fitnesswerten aufgerufen. Ist das Ergebnis der Funktion \emph{True} wird die Ausführung des Algorithmus beendet und die entsprechenden Callback Funktionen \emph{on\_finish()} und \emph{on\_cleanup()} aufgerufen. Liefert die Funktion \emph{False}, wird mit der Selektion, Rekombination und Mutation fortgefahren und der Zyklus startet erneut. Diese Art der Abbruchbedingung ermöglicht eine vielfältige Umsetzungen. Zum Beispiel kann die der Algorithmus beendet werden, wenn der Fitnesswert eines Agenten einen Schwellwert übersteigt, eine gewisse Anzahl an Zyklen bzw. Generationen durchgeführt sind oder wenn die eine gewisse Trainingszeit überschritten ist.

\subsubsection{Serviceklassen}
Ein großes Ziel bei der Entwicklung ist das Einsparen von unnötigem Implementierungsaufwand, was auch für diese Arbeit gilt. Aus diesem Grund wurde die zuvor vorgestellten Interfaces erstellt, welche sowohl von der sequenziellen auch parallelisierten Implementierung verwendet werden. Der Vorteil hierdurch ist, dass ein einfaches Austauschen der beiden Umsetzungen möglich ist und einen einfachen Vergleich ermöglicht. Die eigentliche Implementierung wird sich zwangsläufig an einigen Punkten unterscheiden. Dennoch wird ein großer Teil der \ac{NEAT} Komponenten gleich bleiben, da sich die Funktionen nicht durch eine Parallelisierung ändern. Aus diesem Grund wird ein Großteil dieser Funktionen als Serviceklassen implementiert und keinen internen Zustand besitzen und bieten somit zwei Vorteile. Der erste ist, dass es einfach möglich ist die Funktionen in verschiedenen Implementierungen zu nutzen. Zweitens ermöglicht eine solche Struktur ein einfaches automatisiertes Testen der Implementierung. Dies ist besonders wichtig, da Fehler später bei einem Trainingsverfahren durch die Größe der Population schwer zu lokalisieren sind.
\\\\
Insgesamt werden drei Serviceklassen mit den Namen \emph{GenerationService}, \emph{ReproductionService} und \emph{SpeciesService}, welche Funktionen entsprechend ihrer Benennung übernehmen. Somit befasst sich die erste Klasse mit allen Funktionen welche die ganze Generation betreffen, die zweite mit Funktionen bezüglich dem Erstellen und Modifizieren von Genomen sowie Agenten und die letzte Klasse mit Funktionen bezüglich der verschiedenen Spezies. Im Folgenden wird auf die enthaltenen Funktionen jeder Serviceklasse genauer eingegangen. Allerdings sei hierzu angemerkt, dass keine genauen Implementierungsdetails vorgestellt werden. Die theoretischen Funktionalität ist in den Kapiteln \ref{sec:evolutionary_algos} und  \ref{sec:neat} erläutert. Für die genaue praktische Umsetzung wird auf den veröffentlichen Programmcode verwiesen.
\\\\
Der \emph{ReproductionService} besitzt vor allem Funktionen um neue Genome mittels Reproduktion zu erzeugen oder bestehende zu mutieren. Die erste Funktion heißt \emph{cross\_over()} und setzt die eigentliche Reproduktion um. In ihr werden zwei Elterngenome verwendet um einen Nachkommen zu erzeugen. Grundsätzlich sind sowohl die Umsetzungen dieser Funktion als auch der anderen in diesem Kapitel vorgestellten Funktionen entsprechend der Erläuterung aus den vorherigen Kapiteln. Eine Besonderheit soll an dieser Stelle trotzdem hervorgehoben werden. Ein wichtiges Ziel für ein wiederholbares Ergebnis ist, dass unabhängig von einem Prozessor mit demselben Seed immer dasselbe Ergebnis erzeugt wird. Dies ist mit einem einfachen globalen Zufallsgenerator in einem Verteilten System nicht möglich. Aus diesem Grund wird für jedes Genom ein neuer Zufallsgenerator erzeugt, dessen Seed sich aus den beiden Elterngenomen ergibt, welcher dann für alle Zufallsoperationen verwendet wird, die dieses Genom betreffen. Der hieraus resultierende Vorteil ist, dass unabhängig von dem Prozessor und dessen interner Zustand dieselben Eltern immer denselben Nachkommen produzieren. Der hierbei erstellte Zufallsgenerator wird auch für die Funktionen \emph{mutate\_weights()}, \emph{mutate\_add\_connection()} und \emph{mutate\_add\_node()} verwendet. Hiervon mutiert die erste die Gewichte aller Verbindungen, die zweite fügt eine neue Verbindung und die dritte ein neues Neuron mit den entsprechenden Verbindungen dem Genom hinzu. 
\\\\
Die Klasse \emph{GenerationService} besitzt einige Funktionen zum Erzeugen der initialen Population. Bei der Schnittstelle der Bibliothek wird nur die Anzahl an \emph{Input}- und \emph{Output}-Neuronen übergeben


\subsubsection{Visualisierung}
\subsubsection{Performance Messung}
\subsubsection{Persistenz}
% Reporter?


