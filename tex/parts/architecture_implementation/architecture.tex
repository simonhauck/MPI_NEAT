% !TeX spellcheck = de_DE
\section{Softwarearchitektur}
Im Folgenden wird auf die Architektur der Software, sowie der Implementierung des sequenziellen Verfahrens eingegangen. Das grundsätzliche Ziel ist, dass so viele Komponenten wie möglich von der sequentiellen und parallelisierten Implementierung genutzt werden können, sodass einerseits der programmiertechnische Aufwand möglichst gering ist und andererseits dass ein einfaches Austauschen der beiden Varianten möglich ist. Hierfür wird eine Kombination aus verschiedenen Interfaces sowie Serviceklassen verwendet auf welche  im Rahmen dieses Kapitels genauer eingegangen wird. Zuvor werden noch die verwendeten Datenstrukturen und das Interface für die Bibliothek vorgestellt.
 
\subsubsection{Datenstrukturen}
Die verwendeten Datenstrukturen sind für beide Implementierung identisch und in Abbildung (TODO ABBILDUNG) in einem UML Diagramm dargestellt. Wie zu erkennen ist, besitzen diese keine Funktionen oder Methoden und können so auf jeden Fall für beide Implementierungen genutzt werden. Die eigentliche Funktionalität wird in den später vorgestellten Serviceklassen umgesetzt.
\\\\
Die erste Struktur ist das \emph{Genom}, welches aus Kapitel \ref{sec:neat} bekannt ist und alle Informationen zur Konstruktion eines \ac{KNN} besitzt. Hierfür werden zwei Listen benötigt, wobei die erste alle Neuronen und die zweite alle Verbindungen des \ac{KNN} enthält. Diese Repräsentation entspricht der Kodierung aus Kapitel \ref{subsec:neat_encoding}. 
\\\\
Ein Neuron wird durch die Klasse \emph{Node} repräsentiert und enthält unter anderem den Bias-Wert und die zu verwendende Aktivierungsfunktion, welche für die Berechnungen im \ac{KNN} benötigt werden. Zusätzlich sind einige weitere Werte vorhanden, welche die spätere Implementierungen vereinfachen. Hierzu gehört unter anderem der Neuronentyp, welcher angibt ob es sich um ein \emph{Input}-, \emph{Hidden}- oder \emph{Ouput}-Neuron handelt. Dies vereinfacht die spätere Visualisierung sowie das Einsetzen der Eingabewerte und das Auslesen des Ausgabevektors. Zusätzlich besitzt jedes Neuron eine X-Koordinate, welche die relative X-Position für eine spätere Visualisierung angibt. Die \emph{Input}- und \emph{Output}-Neuronen besitzen die Werte $0$ und $1$. Die \emph{Hidden}-Neuronen werden, wie in Kapitel \ref{subsec:neat_minimal_structure} beschrieben, nur durch strukturelle Mutationen hinzugefügt und dabei zwischen zwei anderen Neuronen platziert. Die X-Koordinate von diesen wird berechnet, indem der Mittelwert der zwei anderen Neuronen gebildet wird. Zuletzt enthält jedes Neuron zur Identifizierung eine ID, welche von den Verbindungen genutzt wird.
\\\\
Eine Verbindung zwischen zwei Neuronen wird im Genom durch eine Instanz der Klasse \emph{Connection} repräsentiert. Jede von diesen enthält zwei Werte, welche die ID des Start- und Zielneurons der Verbindung enthalten. Zusätzlich ist das Gewicht sowie ein Aktivierungsbit gegeben. Letzteres gibt entsprechend der vorgestellten Kodierung an, ob die Verbindung im \ac{KNN} enthalten sein soll. Die letzte Variable ist die Innovationsnummer, welche in Kapitel \ref{subsec:neat_reproduction} beschrieben ist und durch strukturelle Mutationen zugewiesen, und bei der Reproduktion verwendet wird.
\\\\
In Kapitel \ref{sec:evolutionary_algos} ist der Begriff des Individuums eingeführt worden. In dieser Arbeit werden diese durch Instanzen der Klasse \emph{Agent} repräsentiert und sind nach dem Agenten benannt, welcher beim bestärkenden Lernen mit der Umwelt interagiert. Der hierbei erhaltene Fitnesswert, welcher im späteren Verlauf die Selektion maßgeblich beeinflusst, wird in der entsprechenden Variable \emph{fitness} gespeichert. Das Feld \emph{additional\_info} kann Zusatzinformationen enthalten, welche vom Optimierungsproblem stammen können.
\\\\
Die zwei letzten Klassen in diesem Diagramm sind die \emph{Generation} und \emph{Spezies}. Letzteres ist aus dem \ac{NEAT} Kapitel bekannt und ist eine Gruppierung von ähnlichen Individuen bzw. Agenten. Entsprechend der in Kapitel \ref{subsec:neat_species} enthaltenen Definition, ist eine Spezies durch ein Genom repräsentiert und besitzt eine Mitgliederliste. Zusätzlich wird noch der höchste erreichte Fitnesswert von dem besten Mitglied gespeichert, sowie die Generation, in welcher dieser Wert erzielt wurde. Dies wird benötigt, um den Fortschritt zu überwachen, da eine Spezies, bei welcher die Mitglieder keine Steigerung des Fitnesswertes in einer festgelegten Anzahl an Generationen erzielen, nicht für die Reproduktion ausgewählt wird.
\\\\
Die letzte Klasse ist die \emph{Generation}, welche aus Kapitel \ref{subsec:evolutionary_algorithm} bekannt ist. Diese enthält sowohl eine Liste mit allen Individuen als auch der verschiedenen Spezies. Zusätzlich wird diese durch eine Nummer identifiziert, welche angibt wie viele Zyklus aus Evaluation, Selektion, Rekombination und Mutation bereits durchgeführt wurden.

\subsubsection{Künstliches neuronales Netz}
Aus dem Genom wird ein \ac{KNN} erstellt, welches dann im Optimierungsproblem eingesetzt wird. In Kapitel \ref{sec:neuroal_networks} sind der Aufbau und die Funktionsweise von diesen ausführlich erläutert. Prinzipiell können hierfür verschiedene bereits implementierte Bibliotheken verwendet werden, wie beispielsweise Tensorflow \cite{tensorflow2015} oder Pytorch \cite{pytorch2019}. Mit diesen können vor allem rein ebenenweise verbundene \ac{KNN}, wie sie in Kapitel \ref{subsec:network_structures} vorgestellt sind, schnell und einfach erstellt werden. Allerdings können durch \ac{NEAT} auch \ac{KNN} mit \emph{shortcut} Verbindungen entstehen sowie Netze mit Rückkopplungen. Diese können zwar auch mit Tensorflow und Pytorch abgebildet werden aber dies erfordert einen höheren Aufwand. Daher wird in diesem Arbeit eine eigene vereinfachte Implementierung verwendet. Ein Nachteil von diesem Vorgehen ist, dass große Bibliotheken viele Optimierungen durchführen und auch die Verwendung von \acp{GPU} unterstützen. Um hierbei einen Grundstein für solche zukünftigen Erweiterungen zu legen, wird zuerst ein Interface erstellt, welches die grundlegenden Funktionen definiert. Sowohl die in dieser Arbeit verwendete Implementierung, als auch zukünftige Erweiterungen mit Tensorflow oder ähnlichem können dieses nutzen und ermöglichen somit ein einfaches Austauschen der verschiedenen Implementierungen. Das Interface ist in Abbildung (TODO ABBILDUNG) dargestellt und definiert nur drei Funktionen mit den Namen \emph{build()}, \emph{reset()} und \emph{activate()}. Bei der \emph{build()} Funktion wird ein Genom übergeben und hieraus ein \ac{KNN} konstruiert. Die \emph{activate()} Funktion bekommt eine Liste mit Eingabewerte übergeben und produziert den Ausgabevektor des \ac{KNN}. Die Anzahl an Eingabe- und Ausgabewerten ist entsprechend der Anzahl an \emph{Input}- und \emph{Output}-Neuronen im \ac{KNN}. Bei der \emph{reset()} Funktion werden eventuell gespeicherte Ergebnisse, wie sie bei einem Netz mit Rückkopplungen enthalten sein können, entfernt und auf den Startwert zurückgesetzt. 
\\\\
Die in dieser Arbeit verwendete \emph{BasicNeuralNetwork} Implementierung setzt dieses Interface um, mit dem Ziel dass sowohl Netze mit und ohne Rückkopplungen umgesetzt werden können. Da eine genaue Beschreibung der Implementierung nicht von Interesse ist, wird an dieser Stelle nur der oberflächliche Ablauf beschrieben. Bei der \emph{build()} Funktion wird je eine Liste für die \emph{Input}- und \emph{Output}-Neuronen angelegt, in welche die Neuronen mit dem entsprechenden Typen sortiert werden. Eine dritte Liste enthält alle im \ac{KNN} enthaltenen Neuronen. Jedes Neuron speichert zudem eine Liste mit den Verbindungen, welche zu ihm führen und den aktuellen sowie letzten berechneten Wert. Die letzte wichtige Aufgabe dieser Funktion ist, die Reihenfolge festzulegen, in welcher die einzelnen Neuronen aktiviert bzw. deren Zwischenergebnisse berechnet werden. Bei der Reihenfolge ist zu beachten, dass bei einem Netz ohne Rückkopplung jedes vorherige Neuron bis zu den \emph{Input-Neuronen} bereits aktiviert sein muss, da ansonsten das Ergebnis verfälscht wird. Bei Netzen mit Rückkopplung gilt diese Anforderung prinzipiell auch, außer für Verbindungen welche in derselben Schicht oder von Neuronen der nachfolgenden Schichten ausgehen. Dies sind Rückkopplungen welche den zuletzt berechneten Wert zurückgeben, sodass keine Endlosschleife entsteht. 

\subsubsection{Optimierungsproblem}

\subsubsection{Schnittstelle der Bibliothek}
\subsubsection{Serviceklassen}
\subsection{Visualisierung}
\subsection{Performance Messung}
\subsection{Persistenz}


